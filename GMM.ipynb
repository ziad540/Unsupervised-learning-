{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "938f33ed",
   "metadata": {},
   "source": [
    "# 0-Gaussian model \n",
    "## p(xi​)=k=1∑K​πk​N(xi​∣μk​,Σk​)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ab3aa8",
   "metadata": {},
   "source": [
    "## 1-importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13adcd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e7903",
   "metadata": {},
   "source": [
    "## 2- \n",
    "If Σ is not invertible, np.linalg.inv() will crash.(too few points )\n",
    "\n",
    "eps to the diagonal of Σ to push eigen values from zero makes sure Σ is invertible (no sing)\n",
    "\n",
    "## Multivarite gaussian denisty: how like is the given point under the cluster \n",
    "\n",
    "## $\\mathcal{N}(x \\mid \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma|^{1/2}} \\exp\\left(-\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1df72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(X, mean, cov):\n",
    "    \"\"\"\n",
    "    Computes multivariate Gaussian probability density function for all data points.\n",
    "\n",
    "    Parameters:\n",
    "    X    : (N, D) data matrix\n",
    "    mean : (D,) mean vector\n",
    "    cov  : (D, D) covariance matrix\n",
    "\n",
    "    Returns:\n",
    "    pdf  : (N,) probability values\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    # Add small value to diagonal for numerical stability\n",
    "    eps = 1e-6\n",
    "    cov = cov + eps * np.eye(D)\n",
    "    \n",
    "    # Compute determinant and inverse\n",
    "    det = np.linalg.det(cov)\n",
    "    inv = np.linalg.inv(cov)\n",
    "    \n",
    "    # Normalization constant\n",
    "    norm_const = 1.0 / np.sqrt((2 * np.pi) ** D * det)\n",
    "    \n",
    "    # Difference from mean\n",
    "    diff = X - mean\n",
    "    \n",
    "    # Exponent term (Mahalanobis distance)\n",
    "    exponent = -0.5 * np.sum(diff @ inv * diff, axis=1)\n",
    "    \n",
    "    return norm_const * np.exp(exponent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5475e9",
   "metadata": {},
   "source": [
    "## 3-Intializing the parameters\n",
    "## Mean :The center cluster (μk) : starting with random points\n",
    "## Covariance :(Σk) shape depending on the type(full, tied, diagonal, spherical)\n",
    "## Mixing(πk): Gaussian weight intially all = .k since we have no prior knowlege EM will update it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf6e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_gmm(X, K, covariance_type):\n",
    "    \"\"\"\n",
    "    Initializes GMM parameters.\n",
    "\n",
    "    Returns:\n",
    "    pi    : (K,) mixing coefficients\n",
    "    mu    : (K, D) means\n",
    "    sigma : covariance parameters (depends on type)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    \n",
    "    # Mixing coefficients (equal initially)\n",
    "    pi = np.ones(K) / K\n",
    "    \n",
    "    # Randomly choose data points as initial means\n",
    "    indices = np.random.choice(N, K, replace=False)\n",
    "    mu = X[indices]\n",
    "    \n",
    "    # Initialize covariance\n",
    "    if covariance_type == \"full\":\n",
    "        sigma = np.array([np.cov(X.T) for _ in range(K)])\n",
    "        # each cluster can stretch and rotate  ELLIPSE\n",
    "    elif covariance_type == \"tied\":\n",
    "        sigma = np.cov(X.T)\n",
    "        # each cluster has its own shape ELLIPSE same mean\n",
    "    elif covariance_type == \"diag\": \n",
    "        sigma = np.array([np.var(X, axis=0) for _ in range(K)])\n",
    "        ## RECTANGLE (NO ROTATION)\n",
    "    elif covariance_type == \"spherical\":\n",
    "        sigma = np.array([np.var(X) for _ in range(K)])\n",
    "        ##CIRCLE (NO ROTATION TOO)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid covariance type\")\n",
    "    \n",
    "    return pi, mu, sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebceccbf",
   "metadata": {},
   "source": [
    "## 4 E-Step: Computing responsibilities(probability of belonging to each cluster)\n",
    "$$\n",
    "\\gamma_{ik} = \\frac{\\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}\n",
    "{\\sum_{j=1}^{K} \\pi_j \\, \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)} \n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\gamma_{ik}$ = probability that point $x_i$ belongs to cluster $k$  \n",
    "- $\\pi_k$ = mixing coefficient for cluster $k$  \n",
    "- $\\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ = multivariate Gaussian PDF  \n",
    "- $K$ = total number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d3e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(X, pi, mu, sigma, covariance_type):\n",
    "    \"\"\"\n",
    "    E-step: compute responsibilities (soft membership probabilities) for all points.\n",
    "\n",
    "    Parameters:\n",
    "    X : (N, D) data matrix\n",
    "    pi : (K,) mixing coefficients\n",
    "    mu : (K, D) mean vectors\n",
    "    sigma : covariance parameters\n",
    "    covariance_type : 'full', 'tied', 'diag', or 'spherical'\n",
    "\n",
    "    Returns:\n",
    "    gamma : (N, K) responsibility matrix\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = len(pi)\n",
    "    gamma = np.zeros((N, K))\n",
    "    \n",
    "    # Loop over clusters\n",
    "    for k in range(K):\n",
    "        # Select correct covariance for this type\n",
    "        if covariance_type == \"full\":\n",
    "            cov = sigma[k]\n",
    "        elif covariance_type == \"tied\":\n",
    "            cov = sigma\n",
    "##for all data since all use same covariance matrix only mean differ \n",
    "        elif covariance_type == \"diag\":\n",
    "            cov = np.diag(sigma[k])\n",
    "##vector of variance along each feature \n",
    "\n",
    "        elif covariance_type == \"spherical\":\n",
    "            cov = np.eye(D) * sigma[k]\n",
    "##identity matrix scaled by variance\n",
    "        else:\n",
    "            raise ValueError(\"Invalid covariance_type\")\n",
    "        \n",
    "        # Compute probability of all points under this Gaussian\n",
    "        gamma[:, k] = pi[k] * gaussian_pdf(X, mu[k], cov)\n",
    "    \n",
    "    # Normalize across clusters to get probabilities\n",
    "    gamma /= gamma.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fcb025",
   "metadata": {},
   "source": [
    "## 5 M-step updating the parameters\n",
    "# Gaussian Mixture Model (GMM) Equations - M-step\n",
    "\n",
    "### Effective number of points per cluster:\n",
    "$$\n",
    "N_k = \\sum_{i=1}^{N} \\gamma_{ik}\n",
    "$$\n",
    "\n",
    "### Mixing coefficients:\n",
    "$$\n",
    "\\pi_k = \\frac{N_k}{N}\n",
    "$$\n",
    "\n",
    "### Means:\n",
    "$$\n",
    "\\mu_k = \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma_{ik} x_i\n",
    "$$\n",
    "\n",
    "### Covariances:\n",
    "\n",
    "**Full covariance (general case):**\n",
    "$$\n",
    "\\Sigma_k = \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T\n",
    "$$\n",
    "\n",
    "**Tied covariance (shared across clusters):**\n",
    "$$\n",
    "\\Sigma_{\\text{tied}} = \\frac{1}{N} \\sum_{k=1}^{K} \\sum_{i=1}^{N} \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T\n",
    "$$\n",
    "\n",
    "**Diagonal covariance (no correlation, axes-aligned):**\n",
    "$$\n",
    "\\Sigma_k^{\\text{diag}} = \\text{diag} \\Bigg( \\frac{1}{N_k} \\sum_{i=1}^{N} \\gamma_{ik} (x_i - \\mu_k)^2 \\Bigg)\n",
    "$$\n",
    "\n",
    "**Spherical covariance (same variance in all directions):**\n",
    "$$\n",
    "\\sigma_k^{\\text{spherical}} = \\frac{1}{D N_k} \\sum_{i=1}^{N} \\gamma_{ik} \\|x_i - \\mu_k\\|^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4daf152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(X, gamma, covariance_type):\n",
    "    \"\"\"\n",
    "    M-step: update GMM parameters based on responsibilities.\n",
    "\n",
    "    Parameters:\n",
    "    X : (N, D) data matrix\n",
    "    gamma : (N, K) responsibilities\n",
    "    covariance_type : 'full', 'tied', 'diag', 'spherical'\n",
    "\n",
    "    Returns:\n",
    "    pi : updated mixing coefficients\n",
    "    mu : updated means\n",
    "    sigma : updated covariances\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = gamma.shape[1]\n",
    "    \n",
    "    # Effective number of points per cluster\n",
    "    Nk = gamma.sum(axis=0)\n",
    "    \n",
    "    # Update mixing coefficients\n",
    "    pi = Nk / N\n",
    "    \n",
    "    # Update means\n",
    "    mu = (gamma.T @ X) / Nk[:, None]\n",
    "    \n",
    "    # Update covariances\n",
    "    if covariance_type == \"full\":\n",
    "        sigma = np.zeros((K, D, D))\n",
    "        for k in range(K):\n",
    "            diff = X - mu[k]\n",
    "            sigma[k] = (gamma[:, k][:, None] * diff).T @ diff / Nk[k]\n",
    "    \n",
    "    elif covariance_type == \"tied\":\n",
    "        sigma = np.zeros((D, D))\n",
    "        for k in range(K):\n",
    "            diff = X - mu[k]\n",
    "            sigma += (gamma[:, k][:, None] * diff).T @ diff\n",
    "        sigma /= N\n",
    "    \n",
    "    elif covariance_type == \"diag\":\n",
    "        sigma = np.zeros((K, D))\n",
    "        for k in range(K):\n",
    "            diff = X - mu[k]\n",
    "            sigma[k] = (gamma[:, k][:, None] * diff**2).sum(axis=0) / Nk[k]\n",
    "    \n",
    "    elif covariance_type == \"spherical\":\n",
    "        sigma = np.zeros(K)\n",
    "        for k in range(K):\n",
    "            diff = X - mu[k]\n",
    "            sigma[k] = (gamma[:, k] * np.sum(diff**2, axis=1)).sum() / (Nk[k] * D)\n",
    "    \n",
    "    return pi, mu, sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd4640",
   "metadata": {},
   "source": [
    "## 6-log likelihood(how well current parameters fit data)\n",
    "$$\n",
    "\\log L = \\sum_{i=1}^{N} \\log \\Bigg( \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\Bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d57c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(X, pi, mu, sigma, covariance_type):\n",
    "    \"\"\"\n",
    "    Compute total log-likelihood of the data under current GMM parameters.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    K = len(pi)\n",
    "    likelihood = np.zeros((N, K))\n",
    "    \n",
    "    for k in range(K):\n",
    "        if covariance_type == \"full\":\n",
    "            cov = sigma[k]\n",
    "        elif covariance_type == \"tied\":\n",
    "            cov = sigma\n",
    "        elif covariance_type == \"diag\":\n",
    "            cov = np.diag(sigma[k])\n",
    "        elif covariance_type == \"spherical\":\n",
    "            cov = np.eye(D) * sigma[k]\n",
    "        likelihood[:, k] = pi[k] * gaussian_pdf(X, mu[k], cov)\n",
    "    \n",
    "    log_likelihood = np.sum(np.log(likelihood.sum(axis=1)))\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9f1d5",
   "metadata": {},
   "source": [
    "## 6 : Full EM Algorithm \n",
    "## E-step → M-step → Log-likelihood check reapeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f8facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_fit(X, K, covariance_type=\"full\", max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Fits a Gaussian Mixture Model to data using EM algorithm.\n",
    "\n",
    "    Returns:\n",
    "    pi : mixing coefficients\n",
    "    mu : cluster means\n",
    "    sigma : cluster covariances\n",
    "    gamma : responsibilities\n",
    "    log_likelihoods : list of log-likelihoods per iteration\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    pi, mu, sigma = initialize_gmm(X, K, covariance_type)\n",
    "    log_likelihoods = []\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # E-step: compute responsibilities\n",
    "        gamma = e_step(X, pi, mu, sigma, covariance_type)\n",
    "        \n",
    "        # M-step: update parameters\n",
    "        pi, mu, sigma = m_step(X, gamma, covariance_type)\n",
    "        \n",
    "        # Log-likelihood\n",
    "        ll = compute_log_likelihood(X, pi, mu, sigma, covariance_type)\n",
    "        log_likelihoods.append(ll)\n",
    "        \n",
    "        # Check convergence\n",
    "        if iteration > 0 and abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
    "            print(f\"Converged at iteration {iteration}\")\n",
    "            break\n",
    "    \n",
    "    return pi, mu, sigma, gamma, log_likelihoods\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
